{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\n"
     ]
    }
   ],
   "source": [
    "text = \"Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happili']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"happily\"]\n",
    "stemmer = PorterStemmer()\n",
    "porter_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(porter_stemmed)\n",
    "len(porter_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/I301903/nltk_data'\n    - '/Users/I301903/opt/anaconda3/nltk_data'\n    - '/Users/I301903/opt/anaconda3/share/nltk_data'\n    - '/Users/I301903/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=114'>115</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=115'>116</a>\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=116'>117</a>\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=126'>127</a>\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=127'>128</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=128'>129</a>\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=129'>130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=130'>131</a>\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=131'>132</a>\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=95'>96</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=96'>97</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=97'>98</a>\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=98'>99</a>\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=103'>104</a>\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=104'>105</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=105'>106</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py?line=106'>107</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=746'>747</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=748'>749</a>\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=749'>750</a>\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=751'>752</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=752'>753</a>\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=872'>873</a>\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=874'>875</a>\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=875'>876</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=876'>877</a>\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=877'>878</a>\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=878'>879</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=580'>581</a>\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=581'>582</a>\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///Users/I301903/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py?line=582'>583</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/I301903/nltk_data'\n    - '/Users/I301903/opt/anaconda3/nltk_data'\n    - '/Users/I301903/opt/anaconda3/share/nltk_data'\n    - '/Users/I301903/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[0;32m----> 2\u001b[0m porter_stemmed \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens\u001b[49m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(porter_stemmed)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(porter_stemmed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "porter_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(porter_stemmed)\n",
    "len(porter_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['veri', 'order', 'and', 'method', 'he', 'look', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'tick', 'a', 'sonor', 'sermon', 'under', 'his', 'flap', 'newli', 'bought', 'waist-coat', ',', 'as', 'though', 'it', 'pit', 'it', 'graviti', 'and', 'longev', 'against', 'the', 'leviti', 'and', 'evanesc', 'of', 'the', 'brisk', 'fire', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# snowball stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "snowball_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(snowball_stemmed)\n",
    "len(snowball_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'token': tokens, 'porter_stemmed': porter_stemmed, 'snowball_stemmed': snowball_stemmed})\n",
    "df = df[['token', 'porter_stemmed', 'snowball_stemmed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>porter_stemmed</th>\n",
       "      <th>snowball_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>very</td>\n",
       "      <td>veri</td>\n",
       "      <td>veri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orderly</td>\n",
       "      <td>orderli</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>methodical</td>\n",
       "      <td>method</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>looked</td>\n",
       "      <td>look</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ticking</td>\n",
       "      <td>tick</td>\n",
       "      <td>tick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sonorous</td>\n",
       "      <td>sonor</td>\n",
       "      <td>sonor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>his</td>\n",
       "      <td>hi</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flapped</td>\n",
       "      <td>flap</td>\n",
       "      <td>flap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>newly</td>\n",
       "      <td>newli</td>\n",
       "      <td>newli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>pitted</td>\n",
       "      <td>pit</td>\n",
       "      <td>pit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>its</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gravity</td>\n",
       "      <td>graviti</td>\n",
       "      <td>graviti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>longevity</td>\n",
       "      <td>longev</td>\n",
       "      <td>longev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>levity</td>\n",
       "      <td>leviti</td>\n",
       "      <td>leviti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>evanescence</td>\n",
       "      <td>evanesc</td>\n",
       "      <td>evanesc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token porter_stemmed snowball_stemmed\n",
       "0          very           veri             veri\n",
       "1       orderly        orderli            order\n",
       "3    methodical         method           method\n",
       "5        looked           look             look\n",
       "18      ticking           tick             tick\n",
       "20     sonorous          sonor            sonor\n",
       "23          his             hi              his\n",
       "24      flapped           flap             flap\n",
       "25        newly          newli            newli\n",
       "32       pitted            pit              pit\n",
       "33          its             it               it\n",
       "34      gravity        graviti          graviti\n",
       "36    longevity         longev           longev\n",
       "39       levity         leviti           leviti\n",
       "41  evanescence        evanesc          evanesc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.token != df.porter_stemmed) | (df.token != df.snowball_stemmed)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
